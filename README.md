# Perceptual_confounds

## 2021-02-05 - Rhodri Cusack, Cliona O'Doherty, Hannah Craddock
Selection of stimuli.

## 2022-11-16
COD: I am refactoring the repo, and trying to organise things better for clarity of what steps were taken in order. This means that **paths and imports may be wrong with current restructuring of the folder and code may not run**. If needed to be reproduced, I would recommend following notes step by step and adjusting paths as needed. Lots of code here also seems redundant.

Figures in this repo are confusing and uncertain, see https://docs.google.com/presentation/d/1KjdCezcQwh54wjfxzqFlVMTYm-SA2fcW7J3K2Zgtb50/edit#slide=id.gbb44f3f1ed_0_157 for overall summary of the metrics used to decide final stimuli.

1. perceptual_measures
    crop_and_perceptualmeasures.py
        * crops movies to be 22.5 s long according to specification in movie_times.csv. Controls the framerate and aspect ratio while doing so.
        * calculate the global contrast factor and root mean square difference for each movie clip (measures defined in adjacent scripts, import structure could be better)
        * contrast_sensitivity_func_matlab was run by HC to measure the energy of videos, not sure what's done here or how to run.
    
    outputs of this step are found in step 2./inputs folders, structured as .csv

2. event_structuring
    create_events.py
        * takes the perceptual confound .csv files and elan tagged files (generated by independent tagging process and saved here) and converts them to BIDS events.tsv files. This is to enable the use of nilearn's make_first_level_design_matrix() function.
        * outputs events.pickle files which will be used in step 3.

3. design_modelling
    model_design_matrix.py
        * has functions for generating a simulated design matrix based on the perceptual measures from the videos, and the elan tags (i.e. from events files output by create_events.py)
        * some functions here for efficiency as well, but not sure if they are used/relevant
    
    break_make_first_level_design_matrix.py
        * RC used this as illustration for big bug he found in nilearn function if two events of the same trial type overlap, and end exactly simultaneously, the modelling only registers the end of one, and ramps the rest of the time. 
        * solved this big issue by installing latest version of nilearn from github source.

4. design_corr_and_cluster
    various scripts
        * messy folder with a few different scripts doing the same thing. HC and COD seemed to do clustering themselves each. COD began the proccess, and issues appeared with differences between the convolved and noncovolved design matrices, HC took over from COD and COD worked on MDS instead. Lots of plots saved, not sure if any are hugely relevant or informative.
        * what's important from this step is that we clustered the design matrix correlation matrices, and used the resultant dendrograms to form contrasts of interest on which to optimise our semantic spread.

5. bootstrap
    bootstrapping_mds.py
        * will calculate multidimensional scaling on the various events files by constructing a resampled design matrix, then bootstrap across movies to generate a confidence ellipse about each regressor in the design.
        * uses various contrasts as specified in contrast_list.py

6. efficiency
    A. model_efficiency.py
        * measures efficiency and iteratively drops movies depending on the zstat returned by various contrasts
        * fits a model based on simulated fMRI data, including noise generated with bold_noise.py
        * calculates the z_map using nilearn compute_contrast (contrasts are based on contrast_list.py)
        * generates various distribution plots for each of the contrasts, tells us how detectable they are.
    B. summarize_select_movies.py
        * should pick up on the output of model_efficiency.py (in line 310)
        * then generates a report on what the most common combination of 8 movies are - i.e. which ones should we decide upon

    single_column_efficiency.py
        * RC worked on this script to help solve things limiting efficiency: columns where an event rarely appears, appears all the time, or appears/disappears at a temporal frequency that we have no sensitivity to (e.g., very fast or > 1.5 minutes) and also colinear columns
        * single_efficiency.jpg shows (blue bars) single regressor plus confound columns contrasts [1,0,...,0] and orange bars for a contrast with all regressors included (to see how good a column is with friends included). Orange bars being smaller than blue is good.
        * then tried to improve power/efficiency by collapsing across columns, shown in bespoke_efficiency.jpg. and bespoke_timecourse.jpg. RC used his own efficiency function and produced test_efficiency.py to see if it was doing the right thing (it is)